{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.chdir((os.getcwd()+os.sep+\"mnisttxt\"))\n",
    "#Load train & test datasets\n",
    "for digit in range(10):\n",
    "    if (digit == 0):\n",
    "        train = np.loadtxt(\"train\"+str(digit)+\".txt\",dtype=np.float64)\n",
    "        labels = np.full((train.shape[0], 1), digit,dtype = np.float64)\n",
    "        train = np.concatenate((train, labels), axis=1)\n",
    "        test = np.loadtxt(\"test\"+str(digit)+\".txt\",dtype=np.float64)\n",
    "        test_labels = np.full((test.shape[0], 1), digit,dtype =np.float64)\n",
    "        test = np.concatenate((test, test_labels), axis=1)\n",
    "    else:\n",
    "        train2 = np.loadtxt(\"train\"+str(digit)+\".txt\",dtype=np.float64)\n",
    "        labels = np.full((train2.shape[0], 1), digit,dtype =np.float64)\n",
    "        train2 = np.concatenate((train2, labels), axis=1)\n",
    "        train = np.concatenate((train, train2), axis=0)\n",
    "        test2 = np.loadtxt(\"test\"+str(digit)+\".txt\",dtype=np.float64)\n",
    "        test_labels = np.full((test2.shape[0], 1), digit,dtype = np.float64)\n",
    "        test2 = np.concatenate((test2, test_labels), axis=1)\n",
    "        test = np.concatenate((test, test2), axis=0)\n",
    "\n",
    "#Shuffle both train and test datasets\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(test)\n",
    "\n",
    "\n",
    "#Split train,test data and labels\n",
    "train_data = train[:,:784]\n",
    "train_labels = train[:,784]\n",
    "train_labels = train_labels.astype('int')\n",
    "\n",
    "test_data = test[:,:784]\n",
    "test_labels = test[:,784]\n",
    "test_labels = test_labels.astype('int')\n",
    "\n",
    "\n",
    "# min-max scale of train and test data\n",
    "np.seterr(all='ignore') # disable possible warnings for 0 divisions\n",
    "X_min = np.min(train_data,axis = 0)\n",
    "X_max = np.max(train_data,axis = 0)\n",
    "max_min = np.subtract(X_max,X_min)\n",
    "train_data = (train_data - X_min) / max_min\n",
    "test_data = (test_data - X_min) / max_min\n",
    "#replace nans(from possible zero divisions) with 0\n",
    "train_data[np.isnan(train_data)] = 0\n",
    "test_data[np.isnan(test_data)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the 10 first digits of train dataset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.imshow(train[i,:784].reshape(28,28), cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('label: %s' % (train_labels[0:10],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "#disable warnings for large float numbers of estimated probabilities(softmax)\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "class Perceptron:\n",
    "\n",
    "    #Activation functions plus Softmax\n",
    "    def tanh(np_array):\n",
    "        #return np.tanh(np_array)\n",
    "        return np.divide((np.subtract(np.exp(np_array),np.exp((-np_array)))),(np.add(np.exp(np_array),np.exp((-np_array))))) \n",
    "\n",
    "    def tanh_gradient(np_array):\n",
    "        return (1 - np.power(Perceptron.tanh(np_array), 2))\n",
    "\n",
    "    def softplus(np_array):\n",
    "        return np.log(np.add(1,np.exp(np_array)))\n",
    "\n",
    "    def softplus_gradient(np_array):\n",
    "        return np.divide(1,(np.add(1,np.exp(-np_array))))\n",
    "\n",
    "    def cosine(np_array):\n",
    "        return np.cos(np_array)\n",
    "\n",
    "    def cosine_gradient(np_array):\n",
    "        return -np.sin(np_array)\n",
    "\n",
    "    def softmax(np_array):\n",
    "        return np.divide(np.exp(np_array), np.sum(np.exp(np_array),axis = 1,keepdims = True))\n",
    "\n",
    "\n",
    "\n",
    "    #Dictionary of activation functions\n",
    "    activation_func = {\n",
    "        \"tanh\": lambda np_array: Perceptron.tanh(np_array),\n",
    "        \"softplus\": lambda np_array: Perceptron.softplus(np_array),\n",
    "        \"cosine\": lambda np_array: Perceptron.cosine(np_array)\n",
    "    }\n",
    "\n",
    "    #Dictionary of gradients of activation functions\n",
    "    activation_grad_func = {\n",
    "        \"tanh\": lambda np_array: Perceptron.tanh_gradient(np_array),\n",
    "        \"softplus\": lambda np_array: Perceptron.softplus_gradient(np_array),\n",
    "        \"cosine\": lambda np_array: Perceptron.cosine_gradient(np_array)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, hidden_layer,epochs,req_term,activation,X,y):\n",
    "        \n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.epochs = epochs\n",
    "        self.req_term = req_term\n",
    "        self.activation = activation\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.categories = len(np.unique(self.y))\n",
    "        self.w1,self.b1,self.w2,self.b2 =  None,None,None,None\n",
    "        self.dw1,self.db1,self.dw2,self.db2 =  None,None,None,None\n",
    "        self.a2,self.z2,self.probs = None,None,None\n",
    "        self.labels = self.labels_onehotvector()\n",
    "        self.alpha = 1\n",
    "        self.cost_function_history = []\n",
    "        self.train_size = self.X.shape[0] \n",
    "       \n",
    "    #Check user's input for pre-defined activation functions\n",
    "    activation = property(operator.attrgetter('_activation'))   \n",
    "    @activation.setter\n",
    "    def activation(self, act):\n",
    "        if act.lower() not in[\"tanh\",\"softplus\",\"cosine\"]:\n",
    "            raise Exception(\"activation must be either 'tanh' or 'softplus' or 'cosine'\")\n",
    "        self._activation = act\n",
    "        \n",
    "    \n",
    "    def weight_init(self):\n",
    "    \n",
    "        '''Random initialization of weights using univariate “normal” (Gaussian) distribution \n",
    "        of mean 0 and variance 1 divided by square root of input size\n",
    "        '''\n",
    "    \n",
    "        #set the seed for reproducibility\n",
    "        np.random.seed(1234)\n",
    "        self.w1 = np.random.randn(self.X.shape[1], self.hidden_layer) / np.sqrt(self.X.shape[1])\n",
    "        self.b1 = np.zeros((1, self.hidden_layer))\n",
    "        self.w2 = np.random.randn(self.hidden_layer,self.categories) / np.sqrt(self.X.shape[1])\n",
    "        self.b2 = np.zeros((1,self.categories))\n",
    "        \n",
    "    \n",
    "    def labels_onehotvector(self):\n",
    "        \n",
    "        #convert output labels to one hot vectors\n",
    "        labels = np.zeros((self.y.shape[0],self.categories))\n",
    "        for idx in range(labels.shape[0]):\n",
    "            for jdx in range(self.categories):\n",
    "                if self.y[idx] == jdx:\n",
    "                    labels[idx,jdx] = 1\n",
    "                    break\n",
    "        return labels\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forprob(self):\n",
    "        \n",
    "        #foreward propagation of the neural network\n",
    "        self.z2 = self.X.dot(self.w1) + self.b1\n",
    "        self.a2 = Perceptron.activation_func[self.activation](self.z2)\n",
    "        z3 = self.a2.dot(self.w2) + self.b2\n",
    "        self.probs = Perceptron.softmax(z3)\n",
    "        self.cost_function()\n",
    "        \n",
    "     \n",
    "    def gradcheck(self):\n",
    "        \n",
    "        #Use numerical gradient to verify correctness of our gradients implementation\n",
    "        print(\"Numerical gradient checking may take several time\")\n",
    "        self.weight_init()\n",
    "        init_grads = np.concatenate([self.w1.flatten(),self.b1.flatten(),self.w2.flatten(),self.b2.flatten()])\n",
    "       \n",
    "        numgrads = np.zeros(init_grads.shape[0])\n",
    "        perturb = np.zeros(init_grads.shape[0])\n",
    "        e = 1E-4\n",
    "        for grad in range(init_grads.shape[0]):\n",
    "            perturb[grad] = e\n",
    "            loss1 =self.numgrad_cost(init_grads - perturb) \n",
    "            loss2 = self.numgrad_cost(init_grads + perturb) \n",
    "            numgrads[grad] = (loss2 - loss1) / (2*e) \n",
    "            perturb[grad] = 0\n",
    "        self.forprob()\n",
    "        self.backprob()\n",
    "        updated_grads = np.concatenate([self.dw1.flatten(),self.db1.flatten(),self.dw2.flatten(),self.db2.flatten()])\n",
    "        diff = updated_grads  - numgrads\n",
    "        max_grad_diff = np.absolute(diff).max()\n",
    "        print(\"Maximum absolute difference between gradients and numerical gradients is:\",max_grad_diff)\n",
    "        print(\"If the above value is really small(<1e-6), then backpropagation is implemented correctly\")\n",
    "        \n",
    "        \n",
    "    def numgrad_cost(self,grads):\n",
    "        \n",
    "        #Compute cost function for numerical gradients\n",
    "        #Unroll gradients\n",
    "        \n",
    "        w1 = grads[:self.X.shape[1]*self.hidden_layer].reshape(self.X.shape[1],self.hidden_layer)\n",
    "        b1 = grads[self.X.shape[1]*self.hidden_layer:(self.X.shape[1]*self.hidden_layer)+self.hidden_layer].reshape(1,self.hidden_layer)\n",
    "        w2 = grads[(self.X.shape[1]*self.hidden_layer)+self.hidden_layer:(self.X.shape[1]*self.hidden_layer)+self.hidden_layer+(self.hidden_layer*self.categories)].reshape(self.hidden_layer,self.categories)\n",
    "        b2 =grads[(self.X.shape[1]*self.hidden_layer)+self.hidden_layer+(self.hidden_layer*self.categories):].reshape(1,self.categories)\n",
    "        #foreward propagation\n",
    "        z2 = self.X.dot(w1) + b1\n",
    "        a2 = Perceptron.activation_func[self.activation](z2)\n",
    "        z3 = a2.dot(w2) + b2\n",
    "        probs = Perceptron.softmax(z3)\n",
    "        log_probs = np.multiply(np.log(probs),self.labels)\n",
    "        cost = log_probs.sum()\n",
    "        cost-= self.req_term/2 * (np.sum(np.square(w1)) + np.sum(np.square(w2)))\n",
    "        return cost\n",
    "        \n",
    "    def backprob(self):\n",
    "        \n",
    "        #back propagation of the neural network \n",
    "        delta3 = np.subtract(self.labels,self.probs)\n",
    "        self.dw2 = (self.a2.T).dot(delta3)\n",
    "        self.db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(self.w2.T) * Perceptron.activation_grad_func[self.activation](self.z2)\n",
    "        self.dw1 = np.dot(self.X.T, delta2)\n",
    "        self.db1 = np.sum(delta2, axis=0)\n",
    " \n",
    "        #Subtract regularization terms for w1,w2\n",
    "        self.dw2 -= self.req_term *  self.w2\n",
    "        self.dw1 -= self.req_term *  self.w1\n",
    "        \n",
    "        # Weights update\n",
    "        self.w1 += self.alpha *  self.dw1 / self.train_size\n",
    "        self.b1 += self.alpha * self.db1 / self.train_size\n",
    "        self.w2 += self.alpha * self.dw2 / self.train_size\n",
    "        self.b2 += self.alpha * self.db2 / self.train_size\n",
    "        \n",
    "        \n",
    "        \n",
    "    def cost_function(self):\n",
    "        \n",
    "        #calculate max log likelihood plus reqularization term as cost function\n",
    "        log_probs = np.multiply(np.log(self.probs),self.labels)\n",
    "        \n",
    "        cost = log_probs.sum()\n",
    "        cost-= self.req_term/2 * (np.sum(np.square(self.w1)) + np.sum(np.square(self.w2)))\n",
    "        self.cost_function_history.append(cost)\n",
    "        \n",
    "    def fit(self,verbose = False):\n",
    "        \n",
    "        #train neural network\n",
    "        self.weight_init()\n",
    "        for epoch in range(self.epochs):\n",
    "            self.forprob()\n",
    "            if (epoch % 20 == 0):\n",
    "                self.alpha = 1\n",
    "            if ((epoch > 1) and (self.cost_function_history[-2]) <= self.cost_function_history[-1]):\n",
    "                self.alpha =  round(self.alpha/1.1,2)\n",
    "            self.backprob()\n",
    "            if(verbose and epoch % 10 == 0):\n",
    "                print(\"Cost funcion after iteration %i: %f\" %(epoch,self.cost_function_history[-1]))\n",
    "           \n",
    "    def predict(self,test_data,test_labels):\n",
    "        \n",
    "        print(\"Hidden Layer size: %i Activation function: %s lamda: %s\" %(self.hidden_layer,self.activation,self.req_term))\n",
    "        print(\"Cost Function value: %f\" %(self.cost_function_history[-1]))\n",
    "        train_preds = np.argmax(self.probs,axis = 1)\n",
    "        train_error = 1.0 - np.sum(train_preds == self.y)/self.train_size\n",
    "        print(\"Error on train data:\", train_error )\n",
    "        \n",
    "        #Predict on test_data using learned parameters w1,b1,w2,b2\n",
    "        z1 = test_data.dot(self.w1) + self.b1\n",
    "        a1 = Perceptron.activation_func[self.activation](z1)\n",
    "        z2 = a1.dot(self.w2) + self.b2\n",
    "        probs = Perceptron.softmax(z2)\n",
    "        test_preds = np.argmax(probs, axis=1)\n",
    "        test_error = 1.0 - np.sum(test_preds == test_labels)/test_data.shape[0]\n",
    "        print(\"Error on test data:\", test_error )\n",
    "\n",
    "# nn_gradcheck = Perceptron(10,2500,0.00,\"tanh\",train_data[:2000,:],train_labels[:2000])\n",
    "# nn_gradcheck.gradcheck()       \n",
    "\n",
    "nn = Perceptron(100,100,0.00,\"tanh\",train_data,train_labels)\n",
    "nn.fit(True)\n",
    "nn.predict(test_data,test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
